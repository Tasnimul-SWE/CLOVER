# -*- coding: utf-8 -*-
"""mlc-model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZXajggtxK1WcmecDylAMF4dgpSUKSZzx
"""

def allele_label(ref, alt):
    mapping = {
            'A-C': 1,
            'A-G': 2,
            'A-T': 3,
            'C-A': 4,
            'C-G': 5,
            'C-T': 6,
            'G-A': 7,
            'G-C': 8,
            'G-T': 9,
            'T-A': 10,
            'T-C': 11,
            'T-G': 12,
    }
    return mapping.get(f"{ref}-{alt}", -1)

def genotype_label(ref, alt, genotype):

    if genotype == "0/0":
        return 0
    else:
        return allele_label(ref, alt)

with open("Non-aggressive_45.vcf", "r") as f_in:
    lines = f_in.readlines()

with open("output.csv", "w") as f_out:
    for line in lines:
        line = line.strip()
        if line.startswith("##"):
            continue
        elif line.startswith("#CHROM"):
            headers = line.split("\t")[9:]
            f_out.write(",".join(["CHROM", "POS", "ID", "REF", "ALT"] + headers) + "\n")
        else:
            tokens = line.split("\t")
            if len(tokens) < 5:
                print(f"Skipping line: {line}")
                continue
            chrom, pos, var_id, ref, alt = tokens[:5]
            genotypes = tokens[9:]

            labels = [str(genotype_label(ref, alt, genotype.split(":")[0])) for genotype in genotypes]
            f_out.write(",".join([chrom, pos, var_id, ref, alt] + labels) + "\n")

import pandas as pd
df = pd.read_csv('output (5).csv')

df

df.loc[df.index.astype(str).str.startswith('A'), 'label'] = 1  # samples starting with 'A' are aggressive

df

df = df.iloc[:, :-1]

df

df_transposed = df.transpose()

df_transposed

df_transposed['label'] = 0

df_transposed

df_transposed = df_transposed.reset_index()

# Move the 'index' column to the end
cols = list(df_transposed.columns)
cols = [col for col in cols if col != 'index'] + ['index']
df_transposed = df_transposed[cols]

# Optionally, rename 'index' column to the name you want
df_transposed  = df_transposed.rename(columns={'index': 'Newindex'})

df_transposed

df_transposed.loc[0:104, 'label'] = df_transposed.loc[0:104, 'Newindex'].apply(lambda x: 1 if str(x).startswith('A') else 0)

df_transposed

df_transposed.loc[df_transposed.index[:105], 'label'] = 1

df_transposed

df_transposed.loc[df_transposed['Newindex'].str.startswith('A'), 'label'] = 1

df_transposed

df_transposed= df_transposed.set_index('Newindex')

df_transposed

df_transposed.index.name = None

df_transposed

df_transposed.to_csv('file.csv', index=False)

def label_sample_with_index(idx, sample_name):
    if idx < 80:
        return 1  # Prostate
    elif sample_name.startswith(('A', 'N')):
        return 1  # Prostate
    else:
        return 0  # Breast

import pandas as pd

# Use a loop or comprehension to apply the function
df_transposed['label1'] = [label_sample_with_index(idx, sample_name) for idx, sample_name in enumerate(df_transposed.index)]

import pandas as pd



df_transposed

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model

encoding_dim = 100

n_features = df_transposed.shape[1] - 1
n_features

input_layer = Input(shape=(n_features,))

encoded = Dense(encoding_dim, activation='relu')(input_layer)

decoded = Dense(n_features, activation='sigmoid')(encoded)

autoencoder = Model(input_layer, decoded)

encoder = Model(input_layer, encoded)

encoded_input = Input(shape=(encoding_dim,))
decoder_layer = autoencoder.layers[-1]
decoder = Model(encoded_input, decoder_layer(encoded_input))

autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

X = df_transposed.drop('label', axis=1)

X = df_transposed.drop('label1', axis=1)

autoencoder.fit(X, X, epochs=100)

encoded_X = encoder.predict(X)

from tensorflow.keras.layers import Dropout
from sklearn.model_selection import train_test_split

input_layer = Input(shape=(encoding_dim,))
x = Dense(64, activation='relu')(input_layer)
x = Dropout(0.5)(x)
x = Dense(32, activation='relu')(x)
output_layer = Dense(1, activation='sigmoid')(x)

model = Model(inputs=input_layer, outputs=output_layer)

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

X_train, X_test, y_train, y_test = train_test_split(encoded_X, df_transposed['label'], test_size=0.2)

model.fit(X_train, y_train, epochs=100)

loss, accuracy = model.evaluate(X_test, y_test)

print('Accuracy:', accuracy)

from sklearn.metrics import classification_report

y_pred = model.predict(X_test)

y_pred = [1 if p > 0.5 else 0 for p in y_pred]

print(classification_report(y_test, y_pred))

from tensorflow.keras.layers import Dropout
from sklearn.model_selection import train_test_split

y = df_transposed[['label', 'label1']]

X_train, X_test, y_train, y_test = train_test_split(encoded_X, y, test_size=0.2)

from tensorflow.keras import regularizers

from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping

from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.optimizers import Adam

input_layer = Input(shape=(encoding_dim,))
x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(input_layer)  # Reduced neurons for simplicity
x = BatchNormalization()(x)
x = Dropout(0.5)(x)

x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)

x = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)
x = BatchNormalization()(x)

output_layer = Dense(2, activation='sigmoid')(x)

output_layer = Dense(2, activation='sigmoid')(x)

model = Model(inputs=input_layer, outputs=output_layer)

optimizer = Adam(learning_rate=0.0005)

model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

callbacks = [reduce_lr, early_stop]

model.fit(X_train, y_train, batch_size=128, epochs=150, validation_split=0.2, callbacks=callbacks)

loss, accuracy = model.evaluate(X_test, y_test)
print('Accuracy:', accuracy)

y_pred = model.predict(X_test)

y_pred = (y_pred > 0.5).astype(int)

from sklearn.metrics import classification_report

for i, label_name in enumerate(y.columns):
    print(f"Classification report for {label_name}:")
    print(classification_report(y_test.iloc[:, i], y_pred[:, i]))

from sklearn.metrics import confusion_matrix

def balanced_accuracy(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)
    return (sensitivity + specificity) / 2

label1_balanced_accuracy = balanced_accuracy(y_test['label'], y_pred[:, 0])
label2_balanced_accuracy = balanced_accuracy(y_test['label1'], y_pred[:, 1])

print("Balanced Accuracy for Label1:", label1_balanced_accuracy)
print("Balanced Accuracy for Label2:", label2_balanced_accuracy)

average_balanced_accuracy = (label1_balanced_accuracy + label2_balanced_accuracy) / 2
print("Average Balanced Accuracy:", average_balanced_accuracy)

from sklearn.metrics import confusion_matrix

def balanced_accuracy(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)
    return (sensitivity + specificity) / 2

# Assuming y_test and y_pred are your true and predicted labels, respectively
label1_balanced_accuracy = balanced_accuracy(y_test['label'], y_pred[:, 0])
print("Balanced Accuracy for Label1:", label1_balanced_accuracy)